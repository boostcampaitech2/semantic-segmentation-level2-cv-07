{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3303e95-32f5-4cfc-bb8c-3667c9304ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.7.1\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla V100-SXM2-32GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "# GPU 사용 가능 여부에 따라 device 정보 저장\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 1   # Mini-batch size\n",
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055b1460-fd51-40e4-ae69-55db3c35882a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of super categories: 10\n",
      "Number of categories: 10\n",
      "Number of annotations: 26240\n",
      "Number of images: 3272\n"
     ]
    }
   ],
   "source": [
    "# dataset_path  = '/opt/ml/segmentation/input/data'\n",
    "# anns_file_path = dataset_path + '/' + 'train_all.json'\n",
    "\n",
    "# # Read annotations\n",
    "# with open(anns_file_path, 'r') as f:\n",
    "#     dataset = json.loads(f.read())\n",
    "\n",
    "# categories = dataset['categories']\n",
    "# anns = dataset['annotations']\n",
    "# imgs = dataset['images']\n",
    "# nr_cats = len(categories)\n",
    "# nr_annotations = len(anns)\n",
    "# nr_images = len(imgs)\n",
    "\n",
    "# # Load categories and super categories\n",
    "# cat_names = []\n",
    "# super_cat_names = []\n",
    "# super_cat_ids = {}\n",
    "# super_cat_last_name = ''\n",
    "# nr_super_cats = 0\n",
    "# for cat_it in categories:\n",
    "#     cat_names.append(cat_it['name'])\n",
    "#     super_cat_name = cat_it['supercategory']\n",
    "#     # Adding new supercat\n",
    "#     if super_cat_name != super_cat_last_name:\n",
    "#         super_cat_names.append(super_cat_name)\n",
    "#         super_cat_ids[super_cat_name] = nr_super_cats\n",
    "#         super_cat_last_name = super_cat_name\n",
    "#         nr_super_cats += 1\n",
    "\n",
    "# print('Number of super categories:', nr_super_cats)\n",
    "# print('Number of categories:', nr_cats)\n",
    "# print('Number of annotations:', nr_annotations)\n",
    "# print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a0db82-b8ee-4a87-af56-c0e926e765ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count annotations\n",
    "# cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "# for ann in anns:\n",
    "#     cat_histogram[ann['category_id']-1] += 1\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "# df = df.sort_values('Number of annotations', 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56fb569-6af7-4c58-a2dd-df2bc5f6c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # category labeling \n",
    "# sorted_temp_df = df.sort_index()\n",
    "\n",
    "# # background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "# sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "# sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64bcfa14-61c3-49c8-aaf5-792f7adaec98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categories</th>\n",
       "      <th>Number of annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Backgroud</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General trash</td>\n",
       "      <td>2782.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paper</td>\n",
       "      <td>9311.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paper pack</td>\n",
       "      <td>659.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metal</td>\n",
       "      <td>562.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Glass</td>\n",
       "      <td>610.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Plastic</td>\n",
       "      <td>3090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Styrofoam</td>\n",
       "      <td>1343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Plastic bag</td>\n",
       "      <td>7643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Battery</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Clothing</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Categories  Number of annotations\n",
       "0       Backgroud                    NaN\n",
       "1   General trash                 2782.0\n",
       "2           Paper                 9311.0\n",
       "3      Paper pack                  659.0\n",
       "4           Metal                  562.0\n",
       "5           Glass                  610.0\n",
       "6         Plastic                 3090.0\n",
       "7       Styrofoam                 1343.0\n",
       "8     Plastic bag                 7643.0\n",
       "9         Battery                   63.0\n",
       "10       Clothing                  177.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d4e498-d38a-4c61-89a3-1d1c0534ade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_names = list(sorted_df.Categories)\n",
    "category_names = (\"Backgroud\", \"General trash\", \"Paper\", \"Paper pack\",\n",
    "                    \"Metal\", \"Glass\", \"Plastic\", \"Styrofoam\",\n",
    "                    \"Plastic bag\", \"Battery\", \"Clothing\")\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # General trash = 1, ... , Cigarette = 10\n",
    "            anns = sorted(anns, key=lambda idx : len(idx['segmentation'][0]), reverse=False)\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks[self.coco.annToMask(anns[i]) == 1] = pixel_value\n",
    "            masks = masks.astype(np.int8)\n",
    "                        \n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            return images, image_infos\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cff3e817-4dfa-4854-8ccb-754dda7d04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.59s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.26s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "dataset_path  = '/opt/ml/segmentation/input/data'\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b50bcb-1c34-4afe-80dc-b16c34c55628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "779f63f7-08f6-46c5-b480-6bcdf98120c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'license': 0, 'url': None, 'file_name': 'batch_01_vt/0003.jpg', 'height': 512, 'width': 512, 'date_captured': None, 'id': 0},)\n"
     ]
    }
   ],
   "source": [
    "for imgs, masks, image_infos in train_loader:\n",
    "    print(image_infos)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d696273c-c873-4ddb-aa19-fe71688917bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for imgs, masks, image_infos in train_loader:\n",
    "#     image_infos = image_infos[0]\n",
    "#     file_dir = f\"/opt/ml/semantic-segmentation-level2-cv-07/input/mmseg/ann_dir/training/{image_infos['id']:04}.png\"\n",
    "#     src_img= f\"/opt/ml/semantic-segmentation-level2-cv-07/input/data/{image_infos['file_name']}\"\n",
    "#     trg_img=f\"/opt/ml/semantic-segmentation-level2-cv-07/input/mmseg/img_dir/training/{image_infos['id']:04}.jpg\"\n",
    "#     shutil.move(src_img, trg_img)\n",
    "#     masks = masks[0].numpy()\n",
    "#     cv2.imwrite(file_dir, masks)\n",
    "    \n",
    "for imgs, masks, image_infos in val_loader:\n",
    "    image_infos = image_infos[0]\n",
    "    # file_dir = f\"/opt/ml/semantic-segmentation-level2-cv-07/input/mmseg/ann_dir/validation/{image_infos['id']:04}.png\"\n",
    "    src_img= f\"/opt/ml/semantic-segmentation-level2-cv-07/input/data/{image_infos['file_name']}\"\n",
    "    trg_img=f\"/opt/ml/semantic-segmentation-level2-cv-07/input/mmseg/img_dir/validation/{image_infos['id']:04}.jpg\"\n",
    "    \n",
    "    shutil.move(src_img, trg_img)\n",
    "    # masks = masks[0].numpy()\n",
    "    # cv2.imwrite(file_dir, masks)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "395799e0-5d84-4b98-9a33-7692545032ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, image_infos in test_loader:\n",
    "    image_infos = image_infos[0]\n",
    "    src_img= f\"/opt/ml/semantic-segmentation-level2-cv-07/input/data/{image_infos['file_name']}\"\n",
    "    trg_img=f\"/opt/ml/semantic-segmentation-level2-cv-07/input/mmseg/img_dir/test/{image_infos['id']:04}.jpg\"\n",
    "    shutil.move(src_img, trg_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cf3cb-c2c5-4f37-8742-86c41abb15ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
